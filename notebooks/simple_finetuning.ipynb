{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "from torch.optim import AdamW, Adam\n",
    "import numpy as np\n",
    "\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from print_color import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"TRANSFORMER_FROM_SCRATCH\"] = \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sits_siam.backbone import TransformerBackbone\n",
    "from sits_siam.head import BertHead, ClassifierHead\n",
    "from sits_siam.utils import SitsDataset\n",
    "from sits_siam.bottleneck import PoolingBottleneck, NDVIWord2VecBottleneck\n",
    "from sits_siam.augment import AddNDVIWeights, RandomChanSwapping, RandomChanRemoval, RandomAddNoise, RandomTempSwapping, RandomTempShift, RandomTempRemoval, AddMissingMask, Normalize, Pipeline, ToPytorchTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed():\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = [0.0656, 0.0948, 0.1094, 0.1507, 0.2372, 0.2673, 0.2866, 0.2946, 0.2679, 0.1985]\n",
    "iqd = [0.0456, 0.0536, 0.0946, 0.0769, 0.0851, 0.1053, 0.1066, 0.1074, 0.1428, 0.1376]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = Pipeline(\n",
    "    [\n",
    "        # AddNDVIWeights(),\n",
    "        RandomChanSwapping(),\n",
    "        RandomChanRemoval(),\n",
    "        RandomAddNoise(0.02),\n",
    "        RandomTempSwapping(max_distance=3),\n",
    "        RandomTempShift(),\n",
    "        AddMissingMask(),\n",
    "        Normalize(\n",
    "            a=median,\n",
    "            b=iqd,\n",
    "        ),\n",
    "        ToPytorchTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = Pipeline(\n",
    "    [\n",
    "        # AddNDVIWeights(),\n",
    "        AddMissingMask(),\n",
    "        Normalize(\n",
    "            a=median,\n",
    "            b=iqd,\n",
    "        ),\n",
    "        ToPytorchTensor()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_df = pd.read_parquet(\"data/california_sits_bert_original.parquet\")\n",
    "\n",
    "train_df = whole_df[whole_df.use_bert==0].reset_index(drop=True)\n",
    "val_df = whole_df[whole_df.use_bert==1].reset_index(drop=True)\n",
    "test_df = whole_df[whole_df.use_bert==2].reset_index(drop=True)\n",
    "\n",
    "train_dataset = SitsDataset(train_df, max_seq_len=45, transform=train_transforms)\n",
    "val_dataset = SitsDataset(val_df, max_seq_len=45, transform=val_transforms)\n",
    "test_dataset = SitsDataset(test_df, max_seq_len=45, transform=val_transforms)\n",
    "\n",
    "del train_df\n",
    "del val_df\n",
    "del test_df\n",
    "del whole_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(pl.LightningModule):\n",
    "    def __init__(self, max_seq_len=40, num_classes=13):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.backbone = TransformerBackbone(max_seq_len=max_seq_len)\n",
    "        # self.backbone = torch.load(\"backbone.pt\", map_location=torch.device('cpu'))\n",
    "        self.bottleneck = PoolingBottleneck()\n",
    "        self.classifier = ClassifierHead(num_classes=num_classes)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.val_f1 = MulticlassF1Score(num_classes=num_classes, average='macro')\n",
    "        self.test_f1 = MulticlassF1Score(num_classes=num_classes, average='macro')\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = input[\"x\"]\n",
    "        doy = input[\"doy\"]\n",
    "        mask = input[\"mask\"]\n",
    "        # weight = input[\"weight\"]\n",
    "\n",
    "        features = self.backbone(x, doy, mask)\n",
    "        features = self.bottleneck(features)\n",
    "        outputs = self.classifier(features)\n",
    "        return outputs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        targets = batch[\"y\"]\n",
    "        outputs = self(batch)\n",
    "\n",
    "        loss = self.criterion(outputs, targets)\n",
    "\n",
    "        # Log loss and F1 score\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        targets = batch[\"y\"]\n",
    "        outputs = self(batch)\n",
    "        loss = self.criterion(outputs, targets)\n",
    "\n",
    "        # Calculate F1 score\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        f1_score = self.val_f1(preds, targets)\n",
    "\n",
    "        # Log loss and F1 score\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_f1\", f1_score, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        targets = batch[\"y\"]\n",
    "        outputs = self(batch)\n",
    "        loss = self.criterion(outputs, targets)\n",
    "\n",
    "        # Calculate F1 score\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        f1_score = self.test_f1(preds, targets)\n",
    "\n",
    "        # Log loss and F1 score\n",
    "        self.log(\"test_loss\", loss)\n",
    "        self.log(\"test_f1\", f1_score)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=1e-3, weight_decay=0.05)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=1024, shuffle=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    filename='best_model',\n",
    "    save_top_k=1,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    # log_every_n_steps=5,\n",
    "    devices=\"auto\",\n",
    "    # accelerator=\"gpu\",\n",
    "    # strategy=\"ddp\",\n",
    "    # sync_batchnorm=False,\n",
    "    # use_distributed_sampler=True,\n",
    "    # precision='16-mixed',\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "\n",
    "model = TransformerClassifier()\n",
    "\n",
    "trainer.fit(model=model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
    "\n",
    "model = TransformerClassifier.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "model = model.eval()\n",
    "\n",
    "trainer.test(model=model, dataloaders=test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
