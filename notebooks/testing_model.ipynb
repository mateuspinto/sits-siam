{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_function(name):\n",
    "    if name == \"relu\":\n",
    "        return F.relu\n",
    "    elif name == \"gelu\":\n",
    "        return F.gelu\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported activation function: {name}\")\n",
    "\n",
    "\n",
    "def get_activation_module(name):\n",
    "    if name == \"relu\":\n",
    "        return nn.ReLU()\n",
    "    elif name == \"gelu\":\n",
    "        return nn.GELU()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported activation function: {name}\")\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute 'Scaled Dot Product Attention' without using the math library.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        d_k = query.size(-1)\n",
    "        # Compute scaled dot-product attention\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(\n",
    "            torch.tensor(d_k, dtype=query.dtype, device=query.device)\n",
    "        )\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask, float(\"-inf\"))\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "\n",
    "        output = torch.matmul(p_attn, value)\n",
    "        return output, p_attn\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Headed Attention module without using built-in attention modules.\n",
    "    Supports parameters: d_model, num_heads, dropout, batch_first.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1, batch_first=True):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        self.linear_layers = nn.ModuleList(\n",
    "            [nn.Linear(d_model, d_model) for _ in range(3)]\n",
    "        )\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "        self.attention = Attention()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n",
    "        if self.batch_first:\n",
    "            batch_size, seq_len, _ = query.size()\n",
    "        else:\n",
    "            seq_len, batch_size, _ = query.size()\n",
    "            # Transpose to batch first\n",
    "            query = query.transpose(0, 1)\n",
    "            key = key.transpose(0, 1)\n",
    "            value = value.transpose(0, 1)\n",
    "\n",
    "        # Linear projections\n",
    "        query, key, value = [\n",
    "            linear(x)\n",
    "            .view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "            .transpose(1, 2)\n",
    "            for linear, x in zip(self.linear_layers, (query, key, value))\n",
    "        ]  # Each tensor is of shape (batch_size, num_heads, seq_len, d_k)\n",
    "\n",
    "        # Prepare masks\n",
    "        if key_padding_mask is not None:\n",
    "            # key_padding_mask: (batch_size, seq_len)\n",
    "            # Expand to (batch_size, 1, 1, seq_len)\n",
    "            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(\n",
    "                2\n",
    "            )  # (batch_size, 1, 1, seq_len)\n",
    "            # Expand to match the number of heads\n",
    "            key_padding_mask = key_padding_mask.expand(-1, self.num_heads, -1, -1)\n",
    "        if attn_mask is not None:\n",
    "            # attn_mask: (seq_len, seq_len)\n",
    "            attn_mask = attn_mask.unsqueeze(0)  # (1, seq_len, seq_len)\n",
    "            attn_mask = attn_mask.expand(batch_size * self.num_heads, -1, -1).view(\n",
    "                batch_size, self.num_heads, seq_len, seq_len\n",
    "            )\n",
    "        # Combine masks\n",
    "        if key_padding_mask is not None and attn_mask is not None:\n",
    "            combined_mask = key_padding_mask | attn_mask\n",
    "        elif key_padding_mask is not None:\n",
    "            combined_mask = key_padding_mask\n",
    "        elif attn_mask is not None:\n",
    "            combined_mask = attn_mask\n",
    "        else:\n",
    "            combined_mask = None\n",
    "\n",
    "        # Apply attention\n",
    "        x, attn = self.attention(\n",
    "            query, key, value, mask=combined_mask, dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        # Concatenate heads and apply final linear layer\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        x = self.output_linear(x)\n",
    "\n",
    "        if not self.batch_first:\n",
    "            x = x.transpose(0, 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        num_heads,\n",
    "        dim_feedforward=2048,\n",
    "        dropout=0.1,\n",
    "        activation=\"relu\",\n",
    "        batch_first=True,\n",
    "    ):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadedAttention(\n",
    "            d_model, num_heads, dropout=dropout, batch_first=batch_first\n",
    "        )\n",
    "        # Feedforward network\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.activation_fn = get_activation_function(activation)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        # Self-attention\n",
    "        attn_output = self.self_attn(\n",
    "            src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "        src = src + self.dropout1(attn_output)\n",
    "        src = self.norm1(src)\n",
    "\n",
    "        # Feedforward network\n",
    "        ff_output = self.linear2(self.dropout(self.activation_fn(self.linear1(src))))\n",
    "        src = src + self.dropout2(ff_output)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [copy.deepcopy(encoder_layer) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, src, mask=None, src_key_padding_mask=None):\n",
    "        output = src\n",
    "\n",
    "        for layer in self.layers:\n",
    "            output = layer(\n",
    "                output, src_mask=mask, src_key_padding_mask=src_key_padding_mask\n",
    "            )\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SitsTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=10,\n",
    "        num_classes=9,\n",
    "        d_model=128,\n",
    "        n_head=16,\n",
    "        n_layers=1,\n",
    "        d_inner=128,\n",
    "        activation=\"relu\",\n",
    "        dropout=0.2,\n",
    "        max_len=366,\n",
    "        max_seq_len=70,\n",
    "        T=1000,\n",
    "        max_temporal_shift=30,\n",
    "    ):\n",
    "        super(SitsTransformer, self).__init__()\n",
    "        self.modelname = self._get_name()\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.mlp_dim = [input_dim, 32, 64, d_model]\n",
    "        layers = []\n",
    "        for i in range(len(self.mlp_dim) - 1):\n",
    "            layers.append(LinLayer(self.mlp_dim[i], self.mlp_dim[i + 1]))\n",
    "        self.mlp1 = nn.Sequential(*layers)\n",
    "\n",
    "        self.inlayernorm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.position_enc = PositionalEncoding(\n",
    "            d_model, max_len=max_len + 2 * max_temporal_shift, T=T\n",
    "        )\n",
    "\n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model, n_head, d_inner, dropout, activation, batch_first=True\n",
    "        )\n",
    "        encoder_norm = nn.LayerNorm(d_model)\n",
    "        self.transformerencoder = TransformerEncoder(\n",
    "            encoder_layer, n_layers, encoder_norm\n",
    "        )\n",
    "\n",
    "        layers = []\n",
    "        decoder = [d_model, 64, 32, num_classes]\n",
    "        for i in range(len(decoder) - 1):\n",
    "            layers.append(nn.Linear(decoder[i], decoder[i + 1]))\n",
    "            if i < (len(decoder) - 2):\n",
    "                layers.extend([nn.BatchNorm1d(decoder[i + 1]), nn.ReLU()])\n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "\n",
    "        self.input_sample = {\n",
    "            \"doy\": torch.randint(1, max_len, (2, self.max_seq_len), dtype=torch.int64),\n",
    "            \"mask\": torch.zeros((2, self.max_seq_len), dtype=torch.bool),\n",
    "            \"weight\": torch.rand((2, self.max_seq_len), dtype=torch.float32),\n",
    "            \"x\": torch.rand((2, self.max_seq_len, input_dim), dtype=torch.float32)\n",
    "        }\n",
    "        self.expected_output_sample = torch.rand((2, num_classes), dtype=torch.float32)\n",
    "\n",
    "    def forward(self, input, is_bert=False):\n",
    "        x = input[\"x\"]\n",
    "        doy = input[\"doy\"]\n",
    "        mask = input[\"mask\"]\n",
    "        weight = input[\"weight\"]\n",
    "\n",
    "        x = self.mlp1(x)\n",
    "\n",
    "        x = self.inlayernorm(x)\n",
    "        x = self.dropout(x + self.position_enc(doy))\n",
    "\n",
    "        x = self.transformerencoder(x, src_key_padding_mask=mask)\n",
    "\n",
    "        if not is_bert:\n",
    "            weight = self.dropout(weight)\n",
    "            weight /= weight.sum(1, keepdim=True)\n",
    "            x = torch.bmm(weight.unsqueeze(1), x).squeeze()\n",
    "\n",
    "        logits = self.decoder(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000, T: int = 10000):\n",
    "        super().__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(T) / d_model))\n",
    "        pe = torch.zeros(max_len + 1, d_model)\n",
    "        pe[1:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[1:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, doy):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            doy: Tensor, shape [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        return self.pe[doy]\n",
    "\n",
    "\n",
    "class LinLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(LinLayer, self).__init__()\n",
    "        self.lin = nn.Linear(in_dim, out_dim)\n",
    "        self.ln = nn.LayerNorm(out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin(x)\n",
    "        x = self.ln(x)\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check consistency of output sample\n",
    "\n",
    "with torch.inference_mode():\n",
    "    model = SitsTransformer()\n",
    "    output = model(model.input_sample)\n",
    "    assert output.shape == model.expected_output_sample.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
